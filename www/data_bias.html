

Data bias significantly impacts decision-making in Data Science. Data is considered biased when the sample in a study/model does not represent 
the population of the phenomenon of interest.
<br><br>
Addressing bias is challenging and lacks a straightforward solution. Merely removing potentitially biased variables may not be feasible, as 
they could also carry predictive value. This approach could potentially leave us with no variables to work with. Additionally, some variables can serve as "proxies"
for sensitive characteristics. For example, if ethnicity or religion are deleted from a model but the names remain, names can inadvertently act as proxies 
for ethnicity due to their cultural associations.
<br><br>
Efforts to mitigate bias include adversarial training and fairness-aware machine learning techniques.
<hr>
